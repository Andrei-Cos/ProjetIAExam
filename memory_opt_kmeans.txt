Method 1 (Convert to 32-bit):

to_32bit <- function(x) {
    if(is.numeric(x)) as.numeric(sprintf("%.7f", x)) else x
}

This converts numeric values to 32-bit format.

Method 2 (Reduce dataset size):

if(nrow(clustering_data) > 50000) {
  set.seed(123)
  clustering_data <- clustering_data %>% 
    sample_n(50000)
}

This includes sampling if the dataset is too large.

Method 3 (Memory-efficient functions):

# Using optimized parameters in kmeans
km_result <- kmeans(scaled_data, centers = k_optimal, nstart = 10, iter.max = 20)

# Using rank.=2 in PCA to limit computations
pca_result <- prcomp(scaled_data, scale. = FALSE, rank. = 2)





TODO:

we should also add by how much when we keep 2-3 variables how much we lose information 




18/12:

Donc pcq est done et normalement il fait du sens

On a data augment aussi el dataset de base

Et ca a l'air d'etre carre


Maintenant potentiellement bah si on a 24 variables bah meme avec le pca bah peut etre le kmeans est influence par une variable a la con qui prend trop de place et ca casse les couilles et donne un résultat mzi


Autre obverservation, on a pris 50k sample et bah dessus on a remarque que y a 3 groupes et que 2 sont sous représentés du coup bah on fait une data augmentation pour essayer de mieux representer les groupes qui sont pas blindes. Mais bon ca peut aussi fosser le jeu. 



TODO: verifier avec le dataset de base ou bien récupérer que quelques variables styles du premier data augment et faire une pca dessus (demandez aux esclaves) et puis compare

UPDATE : Still same nigger shit donc good for me. A vérifier avec les autres







